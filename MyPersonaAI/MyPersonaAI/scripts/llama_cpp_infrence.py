#linux  :  CMAKE_ARGS="-DLLAMA_AVX2=ON -DLLAMA_FMA=ON" pip install llama-cpp-python --force-reinstall --no-cache-dir
#windows : $env:CMAKE_ARGS="-DLLAMA_CUBLAS=on"; pip install llama-cpp-python --force-reinstall --no-cache-dir


#llama-3.2-1b-instruct-q8_0.gguf

from llama_cpp import Llama


llm = Llama(model_path="models/llama-3.2-1b-instruct-q8_0.gguf",
            chat_format="llama-3",n_ctx=4092,
            #n_threads=8,      # Adjust based on your CPU cores
            n_gpu_layers= 32,   # Explicitly disable GPU
            verbose=True)

# åˆå§‹ç³»ç»Ÿæç¤º
messages = [
    {"role": "system", "content": "You are a helpful assistant."}
]

print("ğŸ¤– Chat with LLaMA. Type 'exit' to quit.\n")

while True:
    user_input = input("ğŸ‘¤ You: ")
    if user_input.strip().lower() in ["exit", "quit"]:
        print("ğŸ‘‹ Bye!")
        break

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    messages.append({"role": "user", "content": user_input})

    # è·å–å›å¤
    response = llm.create_chat_completion(messages)

    # æå–æ¨¡å‹å›å¤
    reply = response['choices'][0]['message']['content']
    print(f"ğŸ¤– LLaMA: {reply}\n")

    # æ·»åŠ æ¨¡å‹å›å¤åˆ°ä¸Šä¸‹æ–‡
    messages.append({"role": "assistant", "content": reply})


messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What's the distance from Earth to Mars?"}
]

response = llm.create_chat_completion(messages)
print(response['choices'][0]['message']['content'])



