#linux  :  CMAKE_ARGS="-DLLAMA_AVX2=ON -DLLAMA_FMA=ON" pip install llama-cpp-python --force-reinstall --no-cache-dir
#windows : $env:CMAKE_ARGS="-DLLAMA_CUBLAS=on"; pip install llama-cpp-python --force-reinstall --no-cache-dir


#llama-3.2-1b-instruct-q8_0.gguf

from llama_cpp import Llama


llm = Llama(model_path="models/llama-3.2-1b-instruct-q8_0.gguf",
            chat_format="llama-3",n_ctx=4092,
            n_threads=8,      # Adjust based on your CPU cores
            n_gpu_layers= 16,   # Explicitly disable GPU
            stream=True,
            verbose=True)

# åˆå§‹ç³»ç»Ÿæç¤º
messages = [
    {"role": "system", "content": "You are a helpful assistant."}
]

print("ğŸ¤– Chat with LLaMA. Type 'exit' to quit.\n")

while True:
    user_input = input("ğŸ‘¤ You: ")
    if user_input.strip().lower() in ["exit", "quit"]:
        print("ğŸ‘‹ Bye!")
        break

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    messages.append({"role": "user", "content": user_input})

    # è·å–å›å¤
    print("ğŸ¤– LLaMA: ", end="", flush=True)
    response_stream = llm.create_chat_completion(messages, stream=True)

    reply = ""
    for chunk in response_stream:
        delta = chunk["choices"][0]["delta"]
        content = delta.get("content", "")
        print(content, end="", flush=True)
        reply += content

    print("\n")  # æ¢è¡Œ




